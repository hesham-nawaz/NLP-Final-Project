{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (1.5.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/asharfarooq/Library/Python/3.9/lib/python/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/asharfarooq/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas) (1.21.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/asharfarooq/Library/Python/3.9/lib/python/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (1.21.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: rouge-score in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from rouge-score) (1.21.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/asharfarooq/Library/Python/3.9/lib/python/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: absl-py in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from rouge-score) (1.3.0)\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from rouge-score) (3.6.2)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nltk->rouge-score) (1.0.1)\n",
      "Requirement already satisfied: regex in /Users/asharfarooq/Library/Python/3.9/lib/python/site-packages (from nltk->rouge-score) (2021.4.4)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nltk->rouge-score) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nltk->rouge-score) (4.62.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (3.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.16 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (1.21.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/asharfarooq/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six in /Users/asharfarooq/Library/Python/3.9/lib/python/site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting sumy\n",
      "  Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 4.8 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting pycountry>=18.2.23\n",
      "  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.1 MB 9.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sumy) (3.6.2)\n",
      "Requirement already satisfied: requests>=2.7.0 in /Users/asharfarooq/Library/Python/3.9/lib/python/site-packages (from sumy) (2.23.0)\n",
      "Collecting breadability>=0.1.20\n",
      "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
      "Collecting docopt<0.7,>=0.6.1\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Requirement already satisfied: chardet in /Users/asharfarooq/Library/Python/3.9/lib/python/site-packages (from breadability>=0.1.20->sumy) (3.0.4)\n",
      "Requirement already satisfied: lxml>=2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from breadability>=0.1.20->sumy) (4.6.3)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nltk>=3.0.2->sumy) (4.62.0)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nltk>=3.0.2->sumy) (7.1.2)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nltk>=3.0.2->sumy) (1.0.1)\n",
      "Requirement already satisfied: regex in /Users/asharfarooq/Library/Python/3.9/lib/python/site-packages (from nltk>=3.0.2->sumy) (2021.4.4)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pycountry>=18.2.23->sumy) (56.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/asharfarooq/Library/Python/3.9/lib/python/site-packages (from requests>=2.7.0->sumy) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/asharfarooq/Library/Python/3.9/lib/python/site-packages (from requests>=2.7.0->sumy) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests>=2.7.0->sumy) (2021.5.30)\n",
      "Building wheels for collected packages: breadability, docopt, pycountry\n",
      "  Building wheel for breadability (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21682 sha256=338d667b979c827961d6c2e77acf6001bd43eda81001207232547a13e0564a8d\n",
      "  Stored in directory: /Users/asharfarooq/Library/Caches/pip/wheels/ba/9f/70/7795228568b81b57a8932755938da9fb1f291b0576752604aa\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=73ad7e277be878a5a10ac3e55d9475953dbf7353b3c92ff8f1956a5095a1907b\n",
      "  Stored in directory: /Users/asharfarooq/Library/Caches/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "  Building wheel for pycountry (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681832 sha256=9919ba8db52dfd24ef340089df99c2d85454c9187c9c4e410c37b9c4ed69406b\n",
      "  Stored in directory: /Users/asharfarooq/Library/Caches/pip/wheels/47/15/92/e6dc85fcb0686c82e1edbcfdf80cfe4808c058813fed0baa8f\n",
      "Successfully built breadability docopt pycountry\n",
      "Installing collected packages: docopt, pycountry, breadability, sumy\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: '/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tests/__init__.py'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# installing appropiate libraries and packages\n",
    "\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install rouge-score\n",
    "%pip install matplotlib\n",
    "%pip install sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading full dataset in\n",
    "movies = pd.read_csv('rotten_tomatoes_movies.csv')\n",
    "reviews = pd.read_csv('rotten_tomatoes_critic_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping movies that don't have ground truth\n",
    "filteredMovies = movies.dropna(subset = \"critics_consensus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter: how many movies to conduct the analysis for (CHANGE THIS)\n",
    "to_keep = 1000\n",
    "\n",
    "furtherFilteredMovies = filteredMovies[:to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_to_keep = furtherFilteredMovies[['rotten_tomatoes_link']]\n",
    "\n",
    "# data table for each critic review that has ground truth\n",
    "combined_data = movies_to_keep.merge(reviews, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rotten_tomatoes_link</th>\n",
       "      <th>critic_name</th>\n",
       "      <th>top_critic</th>\n",
       "      <th>publisher_name</th>\n",
       "      <th>review_type</th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Andrew L. Urban</td>\n",
       "      <td>False</td>\n",
       "      <td>Urban Cinefile</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-02-06</td>\n",
       "      <td>A fantasy adventure that fuses Greek mythology...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Louise Keller</td>\n",
       "      <td>False</td>\n",
       "      <td>Urban Cinefile</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-02-06</td>\n",
       "      <td>Uma Thurman as Medusa, the gorgon with a coiff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m/0814255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>FILMINK (Australia)</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-02-09</td>\n",
       "      <td>With a top-notch cast and dazzling special eff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Ben McEachen</td>\n",
       "      <td>False</td>\n",
       "      <td>Sunday Mail (Australia)</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>3.5/5</td>\n",
       "      <td>2010-02-09</td>\n",
       "      <td>Whether audiences will get behind The Lightnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Ethan Alter</td>\n",
       "      <td>True</td>\n",
       "      <td>Hollywood Reporter</td>\n",
       "      <td>Rotten</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-02-10</td>\n",
       "      <td>What's really lacking in The Lightning Thief i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  rotten_tomatoes_link      critic_name  top_critic           publisher_name  \\\n",
       "0            m/0814255  Andrew L. Urban       False           Urban Cinefile   \n",
       "1            m/0814255    Louise Keller       False           Urban Cinefile   \n",
       "2            m/0814255              NaN       False      FILMINK (Australia)   \n",
       "3            m/0814255     Ben McEachen       False  Sunday Mail (Australia)   \n",
       "4            m/0814255      Ethan Alter        True       Hollywood Reporter   \n",
       "\n",
       "  review_type review_score review_date  \\\n",
       "0       Fresh          NaN  2010-02-06   \n",
       "1       Fresh          NaN  2010-02-06   \n",
       "2       Fresh          NaN  2010-02-09   \n",
       "3       Fresh        3.5/5  2010-02-09   \n",
       "4      Rotten          NaN  2010-02-10   \n",
       "\n",
       "                                      review_content  \n",
       "0  A fantasy adventure that fuses Greek mythology...  \n",
       "1  Uma Thurman as Medusa, the gorgon with a coiff...  \n",
       "2  With a top-notch cast and dazzling special eff...  \n",
       "3  Whether audiences will get behind The Lightnin...  \n",
       "4  What's really lacking in The Lightning Thief i...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping data that don't have review values\n",
    "filtered_combined_data = combined_data.dropna(subset = \"review_content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping critic reviews by each movie and then appending together each review for a given given\n",
    "filtered_appended_reviews = \\\n",
    "pd.DataFrame(filtered_combined_data.groupby(\"rotten_tomatoes_link\")['review_content'].apply(lambda x: ','.join(x))).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulling together a table table that represents each movie and the pre_hypothesis for the summary(joining together of all the reviews for a movie)\n",
    "final_data = \\\n",
    "furtherFilteredMovies.merge(filtered_appended_reviews, on = 'rotten_tomatoes_link')[['movie_title', 'critics_consensus', 'review_content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_title</th>\n",
       "      <th>critics_consensus</th>\n",
       "      <th>review_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Percy Jackson &amp; the Olympians: The Lightning T...</td>\n",
       "      <td>Though it may seem like just another Harry Pot...</td>\n",
       "      <td>A fantasy adventure that fuses Greek mythology...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Please Give</td>\n",
       "      <td>Nicole Holofcener's newest might seem slight i...</td>\n",
       "      <td>Like Holofcener's previous pictures, Please Gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>Blake Edwards' bawdy comedy may not score a pe...</td>\n",
       "      <td>10 (1979) is known for its numerical rating sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12 Angry Men (Twelve Angry Men)</td>\n",
       "      <td>Sidney Lumet's feature debut is a superbly wri...</td>\n",
       "      <td>A film with texture, humour and relevance at a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20,000 Leagues Under The Sea</td>\n",
       "      <td>One of Disney's finest live-action adventures,...</td>\n",
       "      <td>[The] embodiment of Disney at his best -- fami...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         movie_title  \\\n",
       "0  Percy Jackson & the Olympians: The Lightning T...   \n",
       "1                                        Please Give   \n",
       "2                                                 10   \n",
       "3                    12 Angry Men (Twelve Angry Men)   \n",
       "4                       20,000 Leagues Under The Sea   \n",
       "\n",
       "                                   critics_consensus  \\\n",
       "0  Though it may seem like just another Harry Pot...   \n",
       "1  Nicole Holofcener's newest might seem slight i...   \n",
       "2  Blake Edwards' bawdy comedy may not score a pe...   \n",
       "3  Sidney Lumet's feature debut is a superbly wri...   \n",
       "4  One of Disney's finest live-action adventures,...   \n",
       "\n",
       "                                      review_content  \n",
       "0  A fantasy adventure that fuses Greek mythology...  \n",
       "1  Like Holofcener's previous pictures, Please Gi...  \n",
       "2  10 (1979) is known for its numerical rating sy...  \n",
       "3  A film with texture, humour and relevance at a...  \n",
       "4  [The] embodiment of Disney at his best -- fami...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the human-summarizations of all the reviews for a movie\n",
    "ground_truth = list(final_data['critics_consensus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the appended reviews for a given movie\n",
    "appended_reviews_pre_hypotheses = list(final_data['review_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hypothesis_textrank = []\n",
    "final_hypothesis_lexrank = []\n",
    "final_hypothesis_lsa = []\n",
    "\n",
    "summarizer_textrank = TextRankSummarizer()\n",
    "summarizer_lexrank = LexRankSummarizer()\n",
    "summarizer_lsa = LsaSummarizer()\n",
    "\n",
    "for each_movie_appended_reviews_hypothesis in appended_reviews_pre_hypotheses:\n",
    "    parser = PlaintextParser.from_string(each_movie_appended_reviews_hypothesis,Tokenizer(\"english\"))\n",
    "\n",
    "    # TextRank\n",
    "    summary_lexrank = summarizer_lexrank(parser.document,2)\n",
    "    summary_textrank = [str(elt) for elt in summary_textrank]\n",
    "    summary_textrank = \" \".join(summary_textrank)\n",
    "    final_hypothesis_textrank.append(summary_textrank)\n",
    "\n",
    "    # LexRank\n",
    "    summary_lexrank = summarizer_lexrank(parser.document,2)\n",
    "    summary_lexrank = [str(elt) for elt in summary_lexrank]\n",
    "    summary_lexrank = \" \".join(summary_lexrank)\n",
    "    final_hypothesis_lexrank.append(summary_lexrank)\n",
    "\n",
    "    # LSA\n",
    "    summary_lsa = summarizer_lsa(parser.document,2)\n",
    "    summary_lsa = [str(elt) for elt in summary_lsa]\n",
    "    summary_lsa = \" \".join(summary_lsa)\n",
    "    final_hypothesis_lsa.append(summary_lsa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "\n",
    "#running the ROUGE evaluation for certain subset of ground truth and hypothesis pairs\n",
    "num_of_iterations = 100\n",
    "all_rouges_1_scores_textrank = []\n",
    "all_rouges_2_scores_textrank = []\n",
    "\n",
    "all_rouges_1_scores_lexrank = []\n",
    "all_rouges_2_scores_lexrank = []\n",
    "\n",
    "all_rouges_1_scores_lsa = []\n",
    "all_rouges_2_scores_lsa = []\n",
    "\n",
    "for i in range(num_of_iterations):\n",
    "    all_scores_obj_textrank = scorer.score(final_hypothesis_textrank[i], ground_truth[i])\n",
    "    all_rouges_1_scores_textrank.append(all_scores_obj_textrank[\"rouge1\"])\n",
    "    all_rouges_2_scores_textrank.append(all_scores_obj_textrank[\"rouge2\"])\n",
    "\n",
    "    all_scores_obj_lexrank = scorer.score(final_hypothesis_lexrank[i], ground_truth[i])\n",
    "    all_rouges_1_scores_lexrank.append(all_scores_obj_lexrank[\"rouge1\"])\n",
    "    all_rouges_2_scores_lexrank.append(all_scores_obj_lexrank[\"rouge2\"])\n",
    "\n",
    "    all_scores_obj_lsa = scorer.score(final_hypothesis_lsa[i], ground_truth[i])\n",
    "    all_rouges_1_scores_lsa.append(all_scores_obj_lsa[\"rouge1\"])\n",
    "    all_rouges_2_scores_lsa.append(all_scores_obj_lsa[\"rouge2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT RANK ALGORITHM METRICS\n",
      "------\n",
      "Rouge 1:\n",
      "The average precision value for the rouge 1 scores is 0.6756509057229748\n",
      "The average recall value for the rouge 1 scores is 0.03072333815284916\n",
      "The average f1 value for the rouge 1 scores is 0.05791317688811271\n",
      "\n",
      "Rouge 2:\n",
      "The average precision value for the rouge 2 scores is 0.207613474483758\n",
      "The average recall value for the rouge 2 scores is 0.008329307502567218\n",
      "The average f1 value for the rouge 2 scores is 0.01577995648762225\n"
     ]
    }
   ],
   "source": [
    "# building up metric results for Text Rank Algorithm\n",
    "\n",
    "# Rouge 1\n",
    "rouge_1_precisions_textrank = []\n",
    "rouge_1_recall_textrank = []\n",
    "rouge_1_f1_textrank = []\n",
    "for each_rouge_1_score in all_rouges_1_scores_textrank:\n",
    "    rouge_1_precisions_textrank.append(each_rouge_1_score.precision)\n",
    "    rouge_1_recall_textrank.append(each_rouge_1_score.recall)\n",
    "    rouge_1_f1_textrank.append(each_rouge_1_score.fmeasure)\n",
    "\n",
    "avg_rouge_1_precision_score_textrank = np.average(rouge_1_precisions_textrank)\n",
    "avg_rouge_1_recall_score_textrank = np.average(rouge_1_recall_textrank)\n",
    "avg_rouge_1_f1_score_textrank = np.average(rouge_1_f1_textrank)\n",
    "\n",
    "# Rouge 2\n",
    "rouge_2_precisions_textrank = []\n",
    "rouge_2_recall_textrank = []\n",
    "rouge_2_f1_textrank = []\n",
    "for each_rouge_2_score in all_rouges_2_scores_textrank:\n",
    "    rouge_2_precisions_textrank.append(each_rouge_2_score.precision)\n",
    "    rouge_2_recall_textrank.append(each_rouge_2_score.recall)\n",
    "    rouge_2_f1_textrank.append(each_rouge_2_score.fmeasure)\n",
    "\n",
    "avg_rouge_2_precision_score_textrank = np.average(rouge_2_precisions_textrank)\n",
    "avg_rouge_2_recall_score_textrank = np.average(rouge_2_recall_textrank)\n",
    "avg_rouge_2_f1_score_textrank = np.average(rouge_2_f1_textrank)\n",
    "\n",
    "print(\"TEXT RANK ALGORITHM METRICS\")\n",
    "print(\"------\")\n",
    "print(\"Rouge 1:\")\n",
    "print(f'The average precision value for the rouge 1 scores is {avg_rouge_1_precision_score_textrank}')\n",
    "print(f'The average recall value for the rouge 1 scores is {avg_rouge_1_recall_score_textrank}')\n",
    "print(f'The average f1 value for the rouge 1 scores is {avg_rouge_1_f1_score_textrank}')\n",
    "print(\"\")\n",
    "print(\"Rouge 2:\")\n",
    "print(f'The average precision value for the rouge 2 scores is {avg_rouge_2_precision_score_textrank}')\n",
    "print(f'The average recall value for the rouge 2 scores is {avg_rouge_2_recall_score_textrank}')\n",
    "print(f'The average f1 value for the rouge 2 scores is {avg_rouge_2_f1_score_textrank}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEX RANK ALGORITHM METRICS\n",
      "------\n",
      "Rouge 1:\n",
      "The average precision value for the rouge 1 scores is 0.6505254436967709\n",
      "The average recall value for the rouge 1 scores is 0.03882336350928963\n",
      "The average f1 value for the rouge 1 scores is 0.07080522339232535\n",
      "\n",
      "Rouge 2:\n",
      "The average precision value for the rouge 2 scores is 0.1875147042404839\n",
      "The average recall value for the rouge 2 scores is 0.010020281580821461\n",
      "The average f1 value for the rouge 2 scores is 0.018394956239432422\n"
     ]
    }
   ],
   "source": [
    "# building up metric results for Lex Rank Algorithm\n",
    "\n",
    "# Rouge 1\n",
    "rouge_1_precisions_lexrank = []\n",
    "rouge_1_recall_lexrank = []\n",
    "rouge_1_f1_lexrank = []\n",
    "for each_rouge_1_score in all_rouges_1_scores_lexrank:\n",
    "    rouge_1_precisions_lexrank.append(each_rouge_1_score.precision)\n",
    "    rouge_1_recall_lexrank.append(each_rouge_1_score.recall)\n",
    "    rouge_1_f1_lexrank.append(each_rouge_1_score.fmeasure)\n",
    "\n",
    "avg_rouge_1_precision_score_lexrank = np.average(rouge_1_precisions_lexrank)\n",
    "avg_rouge_1_recall_score_lexrank = np.average(rouge_1_recall_lexrank)\n",
    "avg_rouge_1_f1_score_lexrank = np.average(rouge_1_f1_lexrank)\n",
    "\n",
    "# Rouge 2\n",
    "rouge_2_precisions_lexrank = []\n",
    "rouge_2_recall_lexrank = []\n",
    "rouge_2_f1_lexrank = []\n",
    "for each_rouge_2_score in all_rouges_2_scores_lexrank:\n",
    "    rouge_2_precisions_lexrank.append(each_rouge_2_score.precision)\n",
    "    rouge_2_recall_lexrank.append(each_rouge_2_score.recall)\n",
    "    rouge_2_f1_lexrank.append(each_rouge_2_score.fmeasure)\n",
    "\n",
    "avg_rouge_2_precision_score_lexrank = np.average(rouge_2_precisions_lexrank)\n",
    "avg_rouge_2_recall_score_lexrank = np.average(rouge_2_recall_lexrank)\n",
    "avg_rouge_2_f1_score_lexrank = np.average(rouge_2_f1_lexrank)\n",
    "\n",
    "print(\"LEX RANK ALGORITHM METRICS\")\n",
    "print(\"------\")\n",
    "print(\"Rouge 1:\")\n",
    "print(f'The average precision value for the rouge 1 scores is {avg_rouge_1_precision_score_lexrank}')\n",
    "print(f'The average recall value for the rouge 1 scores is {avg_rouge_1_recall_score_lexrank}')\n",
    "print(f'The average f1 value for the rouge 1 scores is {avg_rouge_1_f1_score_lexrank}')\n",
    "print(\"\")\n",
    "print(\"Rouge 2:\")\n",
    "print(f'The average precision value for the rouge 2 scores is {avg_rouge_2_precision_score_lexrank}')\n",
    "print(f'The average recall value for the rouge 2 scores is {avg_rouge_2_recall_score_lexrank}')\n",
    "print(f'The average f1 value for the rouge 2 scores is {avg_rouge_2_f1_score_lexrank}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA ALGORITHM METRICS\n",
      "------\n",
      "Rouge 1:\n",
      "The average precision value for the rouge 1 scores is 0.5485955792813172\n",
      "The average recall value for the rouge 1 scores is 0.05929854300898421\n",
      "The average f1 value for the rouge 1 scores is 0.10102388173633196\n",
      "\n",
      "Rouge 2:\n",
      "The average precision value for the rouge 2 scores is 0.12889357037439042\n",
      "The average recall value for the rouge 2 scores is 0.01099356222196345\n",
      "The average f1 value for the rouge 2 scores is 0.0191179406094523\n"
     ]
    }
   ],
   "source": [
    "# building up metric results for LSA Algorithm\n",
    "\n",
    "# Rouge 1\n",
    "rouge_1_precisions_lsa = []\n",
    "rouge_1_recall_lsa = []\n",
    "rouge_1_f1_lsa = []\n",
    "for each_rouge_1_score in all_rouges_1_scores_lsa:\n",
    "    rouge_1_precisions_lsa.append(each_rouge_1_score.precision)\n",
    "    rouge_1_recall_lsa.append(each_rouge_1_score.recall)\n",
    "    rouge_1_f1_lsa.append(each_rouge_1_score.fmeasure)\n",
    "\n",
    "avg_rouge_1_precision_score_lsa = np.average(rouge_1_precisions_lsa)\n",
    "avg_rouge_1_recall_score_lsa = np.average(rouge_1_recall_lsa)\n",
    "avg_rouge_1_f1_score_lsa = np.average(rouge_1_f1_lsa)\n",
    "\n",
    "# Rouge 2\n",
    "rouge_2_precisions_lsa = []\n",
    "rouge_2_recall_lsa = []\n",
    "rouge_2_f1_lsa = []\n",
    "for each_rouge_2_score in all_rouges_2_scores_lsa:\n",
    "    rouge_2_precisions_lsa.append(each_rouge_2_score.precision)\n",
    "    rouge_2_recall_lsa.append(each_rouge_2_score.recall)\n",
    "    rouge_2_f1_lsa.append(each_rouge_2_score.fmeasure)\n",
    "\n",
    "avg_rouge_2_precision_score_lsa = np.average(rouge_2_precisions_lsa)\n",
    "avg_rouge_2_recall_score_lsa = np.average(rouge_2_recall_lsa)\n",
    "avg_rouge_2_f1_score_lsa = np.average(rouge_2_f1_lsa)\n",
    "\n",
    "print(\"LSA ALGORITHM METRICS\")\n",
    "print(\"------\")\n",
    "print(\"Rouge 1:\")\n",
    "print(f'The average precision value for the rouge 1 scores is {avg_rouge_1_precision_score_lsa}')\n",
    "print(f'The average recall value for the rouge 1 scores is {avg_rouge_1_recall_score_lsa}')\n",
    "print(f'The average f1 value for the rouge 1 scores is {avg_rouge_1_f1_score_lsa}')\n",
    "print(\"\")\n",
    "print(\"Rouge 2:\")\n",
    "print(f'The average precision value for the rouge 2 scores is {avg_rouge_2_precision_score_lsa}')\n",
    "print(f'The average recall value for the rouge 2 scores is {avg_rouge_2_recall_score_lsa}')\n",
    "print(f'The average f1 value for the rouge 2 scores is {avg_rouge_2_f1_score_lsa}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
